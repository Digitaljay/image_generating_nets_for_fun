{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "people2whitewalker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXvBpanf2U2B",
        "colab_type": "text"
      },
      "source": [
        "# Превращаем людей в белых ходоков и обратно\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mf9QYh113JX",
        "colab_type": "text"
      },
      "source": [
        "Работа фаната Игры Престолов: что было бы, если в детсятве героев их младенцами забрали белые ходоки? Как бы они выглядели? А как бы выглядили сами ходоки, если бы их судьба сложилась по-другому?\n",
        "Сейчас узнаем."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "709MJad12eRV",
        "colab_type": "text"
      },
      "source": [
        "Для начала установим и импортируем нужные нам библиотеки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8GVKsMMBoo3",
        "colab_type": "code",
        "outputId": "05081050-a73b-47e9-b8bf-43eb11978c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "!pip3 install torch torchvision\n",
        "!pip3 install pillow==4.1.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Requirement already satisfied: pillow==4.1.1 in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow==4.1.1) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P6d96CdBpZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsdtNROh2nHz",
        "colab_type": "text"
      },
      "source": [
        "# Получаем датасеты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tpoMr2B2rID",
        "colab_type": "text"
      },
      "source": [
        "Здесь пришлось повозиться, но в итоге - вот что у меня получилось. Потом решила, что можно уже здесь архив распаковывать, но и эта реализация имеет право на существование, в целом - без разницы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9txhkxgc2_VV",
        "colab_type": "text"
      },
      "source": [
        "Чтобы запустить ноутбук не вникая в смысл путей к файлам и тд - дальше будут некоторые комментарии по поводу путей к файлам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn3W2uR_Ebun",
        "colab_type": "code",
        "outputId": "6709ba1d-686b-4444-bb8e-fbe68d645823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive  #чтобы получить доступ к файлам на нашем гугл диске\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAK2jAVT3hOW",
        "colab_type": "text"
      },
      "source": [
        "Здесь тоже нужно авторизироваться, это нужно, чтобы получить список файлов (картинок) в конкретной папке"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BScdwrdhRXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "local_download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "     os.makedirs(local_download_path)\n",
        "except: pass\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfi8ZQvH3xGX",
        "colab_type": "text"
      },
      "source": [
        "Вот тут внимательно:\n",
        "\n",
        "\n",
        "1.   Открываете папку hodocs на google drive.\n",
        "2.   Вверху, в качестве ссылки видите что-то вроде https://drive.google.com/drive/folders/1nVCY_S_SCJSffvXhg-HNd0jy2RNOuGFb, та часть, которая после folders/ нам и нужна.\n",
        "3.   Прописываем эту часть пути к файлам в одинарных ковычках у file_list_h\n",
        "4. Эту же операцию повторяем с папкой people и file_list_p\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSR6d4etuO0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_list_h = drive.ListFile(\n",
        "       {'q': \"'1nVCY_S_SCJSffvXhg-HNd0jy2RNOuGFb' in parents\"}).GetList()\n",
        "\n",
        "file_list_p = drive.ListFile(\n",
        "       {'q': \"'1hyMLRCOA2ISuaOrmEcu_2eJP3xNFMJiy' in parents\"}).GetList()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_LbRWvnZ4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imsize = 512\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize(imsize),\n",
        "    transforms.ColorJitter(brightness=(0.75,1.25), contrast=(0.75,1.25), saturation=0, hue=(0.75,1.25)), # так как датасеты очень небольшие, так как у ходоков, к сожалению, мало \n",
        "    transforms.CenterCrop(imsize),                                                                       # экранного времени, то делаем ауги=ментацию\n",
        "    transforms.ToTensor()]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5iGUqLKnbBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # переносим вычисления и данные на gpu\n",
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name).convert('RGB')   # есть png картинки, поэтому приходится приводить к RGB\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIfkArvon6CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unloader = transforms.ToPILImage() # тензор в кратинку  \n",
        "\n",
        "plt.ion() \n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "    image = tensor.cpu().clone()   \n",
        "    image = image.squeeze(0)      # функция для отрисовки изображения\n",
        "    image = unloader(image)\n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5HTVFNgrJiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 25   # определим размер батчей"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogCQKeITku8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Эта ячейка работает если папки hodocs и people лежат в корневом каталоге. Если возникает ошибка - поменяйте ваши пути к папкам\n",
        "hodocs=[]\n",
        "for img in file_list_h:\n",
        "  fname = os.path.join(local_download_path, img['title']).split(\"/\")[-1]\n",
        "  style_img = image_loader(\"gdrive/My Drive/hodocs/\"+fname)\n",
        "  hodocs.append(style_img)\n",
        "\n",
        "people=[]\n",
        "for img in file_list_p:\n",
        "  fname = os.path.join(local_download_path, img['title']).split(\"/\")[-1]\n",
        "  style_img = image_loader(\"gdrive/My Drive/people/\"+fname)\n",
        "  people.append(style_img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z8JjQ26sLc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hodocs_loader = DataLoader(hodocs, batch_size=batch_size, shuffle=True)\n",
        "people_loader = DataLoader(people, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q22ZKoE07Kkr",
        "colab_type": "text"
      },
      "source": [
        "Итак, наши изображения получены. Приступаем к самому интересному."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzFCd9zu7SI9",
        "colab_type": "text"
      },
      "source": [
        "# Модели для CycleGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZgfoj_N8IqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import functools\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-xWfket3yZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLayerDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        self.dis_model = nn.Sequential(\n",
        "                     nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "                     nn.LeakyReLU(0.2, True),\n",
        "\n",
        "                     nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "                     nn.BatchNorm2d(128),\n",
        "                     nn.LeakyReLU(0.2,True),\n",
        "\n",
        "                     nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "                     nn.BatchNorm2d(256),\n",
        "                     nn.LeakyReLU(0.2,True),\n",
        "\n",
        "                     nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "                     nn.BatchNorm2d(512),\n",
        "                     nn.LeakyReLU(0.2,True),\n",
        "\n",
        "                     nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        res=self.dis_model(input)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSSRrquL8GDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnetGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # encoder (downsampling)\n",
        "        self.enc_conv0 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1)\n",
        "        self.pool0 = nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,padding=1,stride=2)  # 256 -> 128\n",
        "        self.enc_conv1 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
        "        self.pool1 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1,stride=2) # 128 -> 64\n",
        "        self.enc_conv2 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
        "        self.pool2 = nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1,stride=2) # 64 -> 32\n",
        "        self.enc_conv3 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)\n",
        "        self.pool3 = nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1,stride=2) # 32 -> 16\n",
        "\n",
        "        # bottleneck\n",
        "        self.bottleneck_conv = nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1)\n",
        "\n",
        "        # decoder (upsampling)\n",
        "        self.upsample0 = nn.ConvTranspose2d(in_channels=256,out_channels=256,kernel_size=2, stride=2, padding=0) # 16 -> 32\n",
        "        self.dec_conv0 = nn.Conv2d(in_channels=512,out_channels=128,kernel_size=3,padding=1)\n",
        "        self.upsample1 = nn.ConvTranspose2d(in_channels=128,out_channels=128,kernel_size=2, stride=2, padding=0) # 32 -> 64\n",
        "        self.dec_conv1 = nn.Conv2d(in_channels=256,out_channels=64,kernel_size=3,padding=1)\n",
        "        self.upsample2 = nn.ConvTranspose2d(in_channels=64,out_channels=64,kernel_size=2, stride=2, padding=0)  # 64 -> 128\n",
        "        self.dec_conv2 = nn.Conv2d(in_channels=128,out_channels=32,kernel_size=3,padding=1)\n",
        "        self.upsample3 = nn.ConvTranspose2d(in_channels=32,out_channels=32,kernel_size=2, stride=2, padding=0)  # 128 -> 256\n",
        "        self.dec_conv3 = nn.Conv2d(in_channels=64,out_channels=3,kernel_size=3,padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        c0 = F.leaky_relu(self.enc_conv0(x),0.02)\n",
        "        e0 = self.pool0(c0)\n",
        "        c1 = F.leaky_relu(self.enc_conv1(e0),0.02)\n",
        "        e1 = self.pool1(c1)\n",
        "        c2 = F.leaky_relu(self.enc_conv2(e1),0.02)\n",
        "        e2 = self.pool2(c2)\n",
        "        c3 = F.leaky_relu(self.enc_conv3(e2),0.02)\n",
        "        e3 = self.pool3(c3)\n",
        "\n",
        "        # bottleneck\n",
        "        cb=F.leaky_relu(self.bottleneck_conv(e3),0.02)\n",
        "        b = self.upsample0(cb)\n",
        "\n",
        "        # decoder\n",
        "        conc0 = torch.cat([b, c3],dim=1)\n",
        "        d0 = self.upsample1(F.leaky_relu(self.dec_conv0(conc0),0.02))\n",
        "        conc1 = torch.cat([d0, c2],dim=1)\n",
        "        d1 = self.upsample2(F.leaky_relu(self.dec_conv1(conc1),0.02))\n",
        "        conc2 = torch.cat([d1, c1],dim=1)\n",
        "        d2 = self.upsample3(F.leaky_relu(self.dec_conv2(conc2),0.02))\n",
        "        conc3 = torch.cat([d2, c0],dim=1)\n",
        "        d3 = self.dec_conv3(conc3)  # no activation\n",
        "        return d3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CeU7hOe_fn2",
        "colab_type": "text"
      },
      "source": [
        "Эта штука нам понадобится, когда будем обучать дискриминаторы, чтобы ипользовать ранее сгенерированные изображения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6wYVXGHAGpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sample_from_Pool(object):\n",
        "    def __init__(self):\n",
        "        self.max_elements = 50\n",
        "        self.cur_elements = 0\n",
        "        self.items = []\n",
        "\n",
        "    def __call__(self, in_items):\n",
        "        return_items = []\n",
        "        for in_item in in_items:\n",
        "            if self.cur_elements < self.max_elements:\n",
        "                self.items.append(in_item)\n",
        "                self.cur_elements = self.cur_elements + 1\n",
        "                return_items.append(in_item)\n",
        "            else:\n",
        "                if np.random.ranf() > 0.5:\n",
        "                    idx = np.random.randint(0, self.max_elements)\n",
        "                    tmp = copy.copy(self.items[idx])\n",
        "                    self.items[idx] = in_item\n",
        "                    return_items.append(tmp)\n",
        "                else:\n",
        "                    return_items.append(in_item)\n",
        "        return return_items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sOidQ6e7_yW",
        "colab_type": "text"
      },
      "source": [
        "# Вишенка на торте - функция обучения модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGp7yW-iF_do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "from torch.optim import lr_scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egONakZ3y-gZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(genA,genB,discA,discB,a_loader,b_loader,optimaizerG,optimaizerD,schedulerG,shedulerD,mse,l1,epochs=100,lamda=10,idt_coef=0.5):\n",
        "\n",
        "    a_fake_sample = Sample_from_Pool()\n",
        "    b_fake_sample = Sample_from_Pool()\n",
        "\n",
        "    # Для графиков\n",
        "    discA_losses_total=[]\n",
        "    discB_losses_total=[]\n",
        "    gen_losses_total=[]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        discA_losses=[]\n",
        "        discB_losses=[]\n",
        "        gen_losses=[]\n",
        "        print('* Epoch %d/%d' % (epoch+1, epochs))\n",
        "        for i, (a_real, b_real) in enumerate(zip(a_loader, b_loader)):\n",
        "\n",
        "                # Временно мы не обучаем дискриминаторы, банкир ещё не видел монеты, пусть сначала их придумает фальшивомонетчик\n",
        "                for param in discA.parameters():\n",
        "                    param.requires_grad = False\n",
        "                for param in discB.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "                # Зануляем\n",
        "                optimaizerG.zero_grad()\n",
        "\n",
        "                a_real = Variable(a_real[0])\n",
        "                b_real = Variable(b_real[0])\n",
        "                a_real, b_real = a_real.to(device), b_real.to(device)\n",
        "\n",
        "                # Сгенерируем фальшивых ходока и человека\n",
        "                a_fake = genA(b_real)\n",
        "                b_fake = genB(a_real)\n",
        "\n",
        "                # И попытаеся восстановить оригиналы\n",
        "                a_recon = genA(b_fake)\n",
        "                b_recon = genB(a_fake)\n",
        "\n",
        "                # А так же сделаем ходока из ходока и человека из человека\n",
        "                a_idt = genA(a_real)\n",
        "                b_idt = genB(b_real)\n",
        "\n",
        "                # Посчитаем лосс, насколько новаторскими они получились\n",
        "                a_idt_loss = l1(a_idt, a_real) * lamda * idt_coef\n",
        "                b_idt_loss = l1(b_idt, b_real) * lamda * idt_coef\n",
        "\n",
        "                # Посмотрим, примут ли наши дискриминаторы сгенерированных ходока и человека за настоящих\n",
        "                a_fake_dis = discA(a_fake)\n",
        "                b_fake_dis = discB(b_fake)\n",
        "\n",
        "                # Посмотрим, насколько хорошо подделывют ходоков и людей генераторы\n",
        "                real_label = Variable(torch.ones(a_fake_dis.size())).to(device)\n",
        "                a_gen_loss = mse(a_fake_dis, real_label)\n",
        "                b_gen_loss = mse(b_fake_dis, real_label)\n",
        "\n",
        "                # И посчитаем потери па полном обороте, между изначальной картинкой и восстановленной из генерированной\n",
        "                a_cycle_loss = l1(a_recon, a_real) * lamda\n",
        "                b_cycle_loss = l1(b_recon, b_real) * lamda\n",
        "\n",
        "                # Общий лосс будет считаться как сумма\n",
        "                gen_loss = a_gen_loss + b_gen_loss + a_cycle_loss + b_cycle_loss + a_idt_loss + b_idt_loss\n",
        "                gen_losses.append(gen_loss) # Это нам понадобится для графиков\n",
        "\n",
        "                # Улучшим модели\n",
        "                gen_loss.backward()\n",
        "                optimaizerG.step()\n",
        "\n",
        "                # Теперь проведём серьёзную беседу с дискриминаторами, для начала разморозим веса, иначе так и не научатся\n",
        "                for param in discA.parameters():\n",
        "                    param.requires_grad = True\n",
        "                for param in discB.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "                # Зануляем\n",
        "                optimaizerD.zero_grad()\n",
        "\n",
        "                # Глянем на уже сгенерированные ранее изображения\n",
        "                a_fake = Variable(torch.Tensor(a_fake_sample([a_fake.cpu().data.numpy()])[0]))\n",
        "                b_fake = Variable(torch.Tensor(b_fake_sample([b_fake.cpu().data.numpy()])[0]))\n",
        "                a_fake, b_fake = a_fake.to(device),b_fake.to(device)\n",
        "\n",
        "                # Посмотрим, как дискриминаторы распознают изображения и зададим, что все фейковые - фейк (0), а настоящие - настоящие (1)\n",
        "                a_real_dis = discA(a_real)\n",
        "                a_fake_dis = discA(a_fake)\n",
        "                b_real_dis = discB(b_real)\n",
        "                b_fake_dis = discB(b_fake)\n",
        "                real_label = Variable(torch.ones(a_real_dis.size())).to(device)\n",
        "                fake_label = Variable(torch.zeros(a_fake_dis.size())).to(device)\n",
        "\n",
        "                # Посчитаем MSELoss\n",
        "                a_dis_real_loss = mse(a_real_dis, real_label)\n",
        "                a_dis_fake_loss = mse(a_fake_dis, fake_label)\n",
        "                b_dis_real_loss = mse(b_real_dis, real_label)\n",
        "                b_dis_fake_loss = mse(b_fake_dis, fake_label)\n",
        "\n",
        "                # Считаем общий лосс каждого из дискриминаторов\n",
        "                a_dis_loss = (a_dis_real_loss + a_dis_fake_loss)*0.5\n",
        "                b_dis_loss = (b_dis_real_loss + b_dis_fake_loss)*0.5\n",
        "                discA_losses.append(a_dis_loss)\n",
        "                discB_losses.append(b_dis_loss)\n",
        "\n",
        "                # Оптимизируем!\n",
        "                a_dis_loss.backward()\n",
        "                b_dis_loss.backward()\n",
        "                optimaizerD.step()\n",
        "\n",
        "                # Секундочку на реализацию красивого вывода\n",
        "                print(\"Epoch: (%3d) (%5d/%5d) | Gen Loss:%.2e | Dis Loss:%.2e\" % \n",
        "                                            (epoch+1, i + 1, min(len(a_loader), len(b_loader)),\n",
        "                                                            gen_loss,a_dis_loss+b_dis_loss))\n",
        "\n",
        "                # Меняем шедулеры\n",
        "                schedulerG.step()\n",
        "                schedulerD.step()\n",
        "\n",
        "        # После того, как сеть трагично вылетела после 12 часов вобучения и добавила эти строки, они появились после осознания того, что всё тлен, и от того особенно важны\n",
        "        if (epoch+1)%10==0:\n",
        "                  # сохранить веса нашей нейросети model\n",
        "                  torch.save(genA.state_dict(), \"gdrive/My Drive/leaky_got_genA\"+str(epoch+1)+\".pth\")\n",
        "                  torch.save(genB.state_dict(), \"gdrive/My Drive/leaky_got_genB\"+str(epoch+1)+\".pth\")\n",
        "                  torch.save(discA.state_dict(), \"gdrive/My Drive/leaky_got_discA\"+str(epoch+1)+\".pth\")\n",
        "                  torch.save(discB.state_dict(), \"gdrive/My Drive/leaky_got_discB\"+str(epoch+1)+\".pth\") \n",
        "\n",
        "\n",
        "        #И да, нам ведь нужны красивые графики\n",
        "        gen_losses_total.append(torch.mean(torch.Tensor(gen_losses)))\n",
        "        discA_losses_total.append(torch.mean(torch.Tensor(discA_losses)))  \n",
        "        discB_losses_total.append(torch.mean(torch.Tensor(discB_losses)))  \n",
        "        \n",
        "    return gen_losses_total, discA_losses_total, discB_losses_total\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_--NQukCosz",
        "colab_type": "text"
      },
      "source": [
        "# Ну и самое прекрасное: процесс обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5TftSd2bBCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP1gWZ7XNqv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=40\n",
        "\n",
        "# Зададим сами сети\n",
        "genA = UnetGenerator()\n",
        "genB = UnetGenerator()\n",
        "discA = NLayerDiscriminator()\n",
        "discB = NLayerDiscriminator()\n",
        "\n",
        "# Оптимизаторы\n",
        "optimaizerG = torch.optim.Adam(itertools.chain(genA.parameters(),genB.parameters()), lr=.0002, betas=(0.5, 0.999))\n",
        "optimaizerD = torch.optim.Adam(itertools.chain(discA.parameters(),discB.parameters()), lr=.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Шедулеры\n",
        "schedulerG = torch.optim.lr_scheduler.StepLR(optimaizerG, 40, 0.5)\n",
        "schedulerD = torch.optim.lr_scheduler.StepLR(optimaizerD, 40, 0.5)\n",
        "\n",
        "# Функции потерь\n",
        "mse = nn.MSELoss()\n",
        "l1 = nn.L1Loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHNzLAR4tN8H",
        "colab_type": "code",
        "outputId": "9a7ea74e-232e-472d-f4c7-f5acfccb146b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gen_losses, discA_losses, discB_losses=train(genA,genB,discA,discB,people_loader,hodocs_loader,optimaizerG,optimaizerD,schedulerG,schedulerD,mse,l1,epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* Epoch 1/40\n",
            "Epoch: (  1) (    1/    6) | Gen Loss:1.87e+01 | Dis Loss:1.28e+00\n",
            "Epoch: (  1) (    2/    6) | Gen Loss:1.30e+01 | Dis Loss:5.07e+00\n",
            "Epoch: (  1) (    3/    6) | Gen Loss:1.27e+01 | Dis Loss:2.88e+00\n",
            "Epoch: (  1) (    4/    6) | Gen Loss:1.28e+01 | Dis Loss:2.38e+00\n",
            "Epoch: (  1) (    5/    6) | Gen Loss:9.18e+00 | Dis Loss:6.46e-01\n",
            "Epoch: (  1) (    6/    6) | Gen Loss:1.04e+01 | Dis Loss:8.31e-01\n",
            "* Epoch 2/40\n",
            "Epoch: (  2) (    1/    6) | Gen Loss:1.14e+01 | Dis Loss:8.03e-01\n",
            "Epoch: (  2) (    2/    6) | Gen Loss:1.06e+01 | Dis Loss:5.80e-01\n",
            "Epoch: (  2) (    3/    6) | Gen Loss:5.82e+00 | Dis Loss:3.62e-01\n",
            "Epoch: (  2) (    4/    6) | Gen Loss:1.37e+01 | Dis Loss:3.37e-01\n",
            "Epoch: (  2) (    5/    6) | Gen Loss:1.68e+01 | Dis Loss:3.23e-01\n",
            "Epoch: (  2) (    6/    6) | Gen Loss:4.72e+00 | Dis Loss:5.34e-01\n",
            "* Epoch 3/40\n",
            "Epoch: (  3) (    1/    6) | Gen Loss:1.04e+01 | Dis Loss:3.40e-01\n",
            "Epoch: (  3) (    2/    6) | Gen Loss:7.30e+00 | Dis Loss:3.92e-01\n",
            "Epoch: (  3) (    3/    6) | Gen Loss:7.49e+00 | Dis Loss:6.12e-01\n",
            "Epoch: (  3) (    4/    6) | Gen Loss:1.16e+01 | Dis Loss:3.40e-01\n",
            "Epoch: (  3) (    5/    6) | Gen Loss:1.01e+01 | Dis Loss:5.32e-01\n",
            "Epoch: (  3) (    6/    6) | Gen Loss:6.33e+00 | Dis Loss:5.90e-01\n",
            "* Epoch 4/40\n",
            "Epoch: (  4) (    1/    6) | Gen Loss:6.88e+00 | Dis Loss:3.76e-01\n",
            "Epoch: (  4) (    2/    6) | Gen Loss:1.03e+01 | Dis Loss:2.91e-01\n",
            "Epoch: (  4) (    3/    6) | Gen Loss:1.06e+01 | Dis Loss:2.79e-01\n",
            "Epoch: (  4) (    4/    6) | Gen Loss:8.17e+00 | Dis Loss:2.57e-01\n",
            "Epoch: (  4) (    5/    6) | Gen Loss:6.65e+00 | Dis Loss:3.35e-01\n",
            "Epoch: (  4) (    6/    6) | Gen Loss:1.55e+01 | Dis Loss:4.26e-01\n",
            "* Epoch 5/40\n",
            "Epoch: (  5) (    1/    6) | Gen Loss:6.76e+00 | Dis Loss:3.74e-01\n",
            "Epoch: (  5) (    2/    6) | Gen Loss:8.94e+00 | Dis Loss:4.45e-01\n",
            "Epoch: (  5) (    3/    6) | Gen Loss:8.44e+00 | Dis Loss:2.42e-01\n",
            "Epoch: (  5) (    4/    6) | Gen Loss:7.43e+00 | Dis Loss:1.68e-01\n",
            "Epoch: (  5) (    5/    6) | Gen Loss:6.93e+00 | Dis Loss:3.16e-01\n",
            "Epoch: (  5) (    6/    6) | Gen Loss:4.88e+00 | Dis Loss:3.47e-01\n",
            "* Epoch 6/40\n",
            "Epoch: (  6) (    1/    6) | Gen Loss:6.09e+00 | Dis Loss:4.08e-01\n",
            "Epoch: (  6) (    2/    6) | Gen Loss:4.27e+00 | Dis Loss:2.62e-01\n",
            "Epoch: (  6) (    3/    6) | Gen Loss:5.15e+00 | Dis Loss:3.68e-01\n",
            "Epoch: (  6) (    4/    6) | Gen Loss:4.93e+00 | Dis Loss:3.36e-01\n",
            "Epoch: (  6) (    5/    6) | Gen Loss:5.74e+00 | Dis Loss:3.43e-01\n",
            "Epoch: (  6) (    6/    6) | Gen Loss:6.06e+00 | Dis Loss:6.04e-01\n",
            "* Epoch 7/40\n",
            "Epoch: (  7) (    1/    6) | Gen Loss:3.49e+00 | Dis Loss:4.83e-01\n",
            "Epoch: (  7) (    2/    6) | Gen Loss:4.62e+00 | Dis Loss:4.09e-01\n",
            "Epoch: (  7) (    3/    6) | Gen Loss:5.96e+00 | Dis Loss:2.16e-01\n",
            "Epoch: (  7) (    4/    6) | Gen Loss:5.52e+00 | Dis Loss:4.15e-01\n",
            "Epoch: (  7) (    5/    6) | Gen Loss:4.35e+00 | Dis Loss:4.26e-01\n",
            "Epoch: (  7) (    6/    6) | Gen Loss:4.50e+00 | Dis Loss:2.86e-01\n",
            "* Epoch 8/40\n",
            "Epoch: (  8) (    1/    6) | Gen Loss:4.17e+00 | Dis Loss:1.97e-01\n",
            "Epoch: (  8) (    2/    6) | Gen Loss:3.68e+00 | Dis Loss:2.10e-01\n",
            "Epoch: (  8) (    3/    6) | Gen Loss:3.32e+00 | Dis Loss:2.60e-01\n",
            "Epoch: (  8) (    4/    6) | Gen Loss:4.33e+00 | Dis Loss:1.60e-01\n",
            "Epoch: (  8) (    5/    6) | Gen Loss:4.59e+00 | Dis Loss:1.91e-01\n",
            "Epoch: (  8) (    6/    6) | Gen Loss:4.16e+00 | Dis Loss:1.63e-01\n",
            "* Epoch 9/40\n",
            "Epoch: (  9) (    1/    6) | Gen Loss:4.16e+00 | Dis Loss:1.75e-01\n",
            "Epoch: (  9) (    2/    6) | Gen Loss:4.11e+00 | Dis Loss:1.02e-01\n",
            "Epoch: (  9) (    3/    6) | Gen Loss:4.61e+00 | Dis Loss:3.84e-01\n",
            "Epoch: (  9) (    4/    6) | Gen Loss:2.99e+00 | Dis Loss:2.21e-01\n",
            "Epoch: (  9) (    5/    6) | Gen Loss:3.18e+00 | Dis Loss:2.35e-01\n",
            "Epoch: (  9) (    6/    6) | Gen Loss:3.66e+00 | Dis Loss:1.46e-01\n",
            "* Epoch 10/40\n",
            "Epoch: ( 10) (    1/    6) | Gen Loss:4.11e+00 | Dis Loss:1.65e-01\n",
            "Epoch: ( 10) (    2/    6) | Gen Loss:3.59e+00 | Dis Loss:1.67e-01\n",
            "Epoch: ( 10) (    3/    6) | Gen Loss:4.39e+00 | Dis Loss:9.12e-02\n",
            "Epoch: ( 10) (    4/    6) | Gen Loss:3.73e+00 | Dis Loss:1.22e-01\n",
            "Epoch: ( 10) (    5/    6) | Gen Loss:3.77e+00 | Dis Loss:1.09e-01\n",
            "Epoch: ( 10) (    6/    6) | Gen Loss:3.69e+00 | Dis Loss:1.63e-01\n",
            "* Epoch 11/40\n",
            "Epoch: ( 11) (    1/    6) | Gen Loss:3.85e+00 | Dis Loss:2.23e-01\n",
            "Epoch: ( 11) (    2/    6) | Gen Loss:3.44e+00 | Dis Loss:1.66e-01\n",
            "Epoch: ( 11) (    3/    6) | Gen Loss:3.04e+00 | Dis Loss:2.56e-01\n",
            "Epoch: ( 11) (    4/    6) | Gen Loss:2.81e+00 | Dis Loss:1.96e-01\n",
            "Epoch: ( 11) (    5/    6) | Gen Loss:3.26e+00 | Dis Loss:2.18e-01\n",
            "Epoch: ( 11) (    6/    6) | Gen Loss:3.25e+00 | Dis Loss:1.13e-01\n",
            "* Epoch 12/40\n",
            "Epoch: ( 12) (    1/    6) | Gen Loss:3.73e+00 | Dis Loss:1.88e-01\n",
            "Epoch: ( 12) (    2/    6) | Gen Loss:4.05e+00 | Dis Loss:1.91e-01\n",
            "Epoch: ( 12) (    3/    6) | Gen Loss:3.44e+00 | Dis Loss:1.59e-01\n",
            "Epoch: ( 12) (    4/    6) | Gen Loss:4.29e+00 | Dis Loss:4.36e-01\n",
            "Epoch: ( 12) (    5/    6) | Gen Loss:2.90e+00 | Dis Loss:2.04e-01\n",
            "Epoch: ( 12) (    6/    6) | Gen Loss:3.97e+00 | Dis Loss:2.41e-01\n",
            "* Epoch 13/40\n",
            "Epoch: ( 13) (    1/    6) | Gen Loss:3.19e+00 | Dis Loss:3.22e-01\n",
            "Epoch: ( 13) (    2/    6) | Gen Loss:4.31e+00 | Dis Loss:1.04e-01\n",
            "Epoch: ( 13) (    3/    6) | Gen Loss:3.44e+00 | Dis Loss:1.07e-01\n",
            "Epoch: ( 13) (    4/    6) | Gen Loss:3.57e+00 | Dis Loss:7.00e-02\n",
            "Epoch: ( 13) (    5/    6) | Gen Loss:4.24e+00 | Dis Loss:2.82e-01\n",
            "Epoch: ( 13) (    6/    6) | Gen Loss:2.66e+00 | Dis Loss:1.56e-01\n",
            "* Epoch 14/40\n",
            "Epoch: ( 14) (    1/    6) | Gen Loss:2.97e+00 | Dis Loss:1.54e-01\n",
            "Epoch: ( 14) (    2/    6) | Gen Loss:4.64e+00 | Dis Loss:9.53e-02\n",
            "Epoch: ( 14) (    3/    6) | Gen Loss:2.82e+00 | Dis Loss:1.52e-01\n",
            "Epoch: ( 14) (    4/    6) | Gen Loss:2.36e+00 | Dis Loss:1.91e-01\n",
            "Epoch: ( 14) (    5/    6) | Gen Loss:2.83e+00 | Dis Loss:1.24e-01\n",
            "Epoch: ( 14) (    6/    6) | Gen Loss:3.96e+00 | Dis Loss:6.23e-02\n",
            "* Epoch 15/40\n",
            "Epoch: ( 15) (    1/    6) | Gen Loss:3.40e+00 | Dis Loss:2.20e-01\n",
            "Epoch: ( 15) (    2/    6) | Gen Loss:2.99e+00 | Dis Loss:5.16e-02\n",
            "Epoch: ( 15) (    3/    6) | Gen Loss:4.44e+00 | Dis Loss:1.58e-01\n",
            "Epoch: ( 15) (    4/    6) | Gen Loss:2.47e+00 | Dis Loss:1.14e-01\n",
            "Epoch: ( 15) (    5/    6) | Gen Loss:2.74e+00 | Dis Loss:1.08e-01\n",
            "Epoch: ( 15) (    6/    6) | Gen Loss:3.04e+00 | Dis Loss:5.97e-02\n",
            "* Epoch 16/40\n",
            "Epoch: ( 16) (    1/    6) | Gen Loss:2.87e+00 | Dis Loss:8.04e-02\n",
            "Epoch: ( 16) (    2/    6) | Gen Loss:4.31e+00 | Dis Loss:1.62e-01\n",
            "Epoch: ( 16) (    3/    6) | Gen Loss:2.81e+00 | Dis Loss:5.63e-02\n",
            "Epoch: ( 16) (    4/    6) | Gen Loss:2.99e+00 | Dis Loss:1.62e-01\n",
            "Epoch: ( 16) (    5/    6) | Gen Loss:3.12e+00 | Dis Loss:9.42e-02\n",
            "Epoch: ( 16) (    6/    6) | Gen Loss:3.18e+00 | Dis Loss:5.10e-02\n",
            "* Epoch 17/40\n",
            "Epoch: ( 17) (    1/    6) | Gen Loss:3.47e+00 | Dis Loss:8.16e-02\n",
            "Epoch: ( 17) (    2/    6) | Gen Loss:3.13e+00 | Dis Loss:1.23e-01\n",
            "Epoch: ( 17) (    3/    6) | Gen Loss:5.12e+00 | Dis Loss:7.54e-02\n",
            "Epoch: ( 17) (    4/    6) | Gen Loss:3.86e+00 | Dis Loss:5.52e-02\n",
            "Epoch: ( 17) (    5/    6) | Gen Loss:5.53e+00 | Dis Loss:1.15e-01\n",
            "Epoch: ( 17) (    6/    6) | Gen Loss:3.68e+00 | Dis Loss:1.86e-01\n",
            "* Epoch 18/40\n",
            "Epoch: ( 18) (    1/    6) | Gen Loss:3.11e+00 | Dis Loss:6.56e-02\n",
            "Epoch: ( 18) (    2/    6) | Gen Loss:3.75e+00 | Dis Loss:3.04e-01\n",
            "Epoch: ( 18) (    3/    6) | Gen Loss:3.48e+00 | Dis Loss:1.43e-01\n",
            "Epoch: ( 18) (    4/    6) | Gen Loss:4.13e+00 | Dis Loss:9.09e-02\n",
            "Epoch: ( 18) (    5/    6) | Gen Loss:3.42e+00 | Dis Loss:8.26e-02\n",
            "Epoch: ( 18) (    6/    6) | Gen Loss:2.59e+00 | Dis Loss:2.39e-01\n",
            "* Epoch 19/40\n",
            "Epoch: ( 19) (    1/    6) | Gen Loss:3.38e+00 | Dis Loss:6.45e-02\n",
            "Epoch: ( 19) (    2/    6) | Gen Loss:4.83e+00 | Dis Loss:1.00e-01\n",
            "Epoch: ( 19) (    3/    6) | Gen Loss:2.48e+00 | Dis Loss:1.21e-01\n",
            "Epoch: ( 19) (    4/    6) | Gen Loss:3.80e+00 | Dis Loss:6.31e-02\n",
            "Epoch: ( 19) (    5/    6) | Gen Loss:3.28e+00 | Dis Loss:1.54e-01\n",
            "Epoch: ( 19) (    6/    6) | Gen Loss:4.32e+00 | Dis Loss:1.02e-01\n",
            "* Epoch 20/40\n",
            "Epoch: ( 20) (    1/    6) | Gen Loss:3.11e+00 | Dis Loss:1.59e-01\n",
            "Epoch: ( 20) (    2/    6) | Gen Loss:2.75e+00 | Dis Loss:1.81e-01\n",
            "Epoch: ( 20) (    3/    6) | Gen Loss:3.78e+00 | Dis Loss:9.77e-02\n",
            "Epoch: ( 20) (    4/    6) | Gen Loss:3.01e+00 | Dis Loss:1.63e-01\n",
            "Epoch: ( 20) (    5/    6) | Gen Loss:3.31e+00 | Dis Loss:1.94e-01\n",
            "Epoch: ( 20) (    6/    6) | Gen Loss:3.50e+00 | Dis Loss:8.58e-02\n",
            "* Epoch 21/40\n",
            "Epoch: ( 21) (    1/    6) | Gen Loss:3.61e+00 | Dis Loss:6.62e-02\n",
            "Epoch: ( 21) (    2/    6) | Gen Loss:3.40e+00 | Dis Loss:1.98e-01\n",
            "Epoch: ( 21) (    3/    6) | Gen Loss:2.95e+00 | Dis Loss:9.99e-02\n",
            "Epoch: ( 21) (    4/    6) | Gen Loss:5.17e+00 | Dis Loss:1.05e-01\n",
            "Epoch: ( 21) (    5/    6) | Gen Loss:3.76e+00 | Dis Loss:6.59e-02\n",
            "Epoch: ( 21) (    6/    6) | Gen Loss:2.83e+00 | Dis Loss:1.07e-01\n",
            "* Epoch 22/40\n",
            "Epoch: ( 22) (    1/    6) | Gen Loss:2.89e+00 | Dis Loss:5.83e-02\n",
            "Epoch: ( 22) (    2/    6) | Gen Loss:3.02e+00 | Dis Loss:1.10e-01\n",
            "Epoch: ( 22) (    3/    6) | Gen Loss:3.42e+00 | Dis Loss:8.75e-02\n",
            "Epoch: ( 22) (    4/    6) | Gen Loss:3.24e+00 | Dis Loss:1.67e-01\n",
            "Epoch: ( 22) (    5/    6) | Gen Loss:3.73e+00 | Dis Loss:7.14e-02\n",
            "Epoch: ( 22) (    6/    6) | Gen Loss:3.85e+00 | Dis Loss:1.05e-01\n",
            "* Epoch 23/40\n",
            "Epoch: ( 23) (    1/    6) | Gen Loss:3.75e+00 | Dis Loss:1.22e-01\n",
            "Epoch: ( 23) (    2/    6) | Gen Loss:3.47e+00 | Dis Loss:6.42e-02\n",
            "Epoch: ( 23) (    3/    6) | Gen Loss:2.59e+00 | Dis Loss:1.71e-01\n",
            "Epoch: ( 23) (    4/    6) | Gen Loss:3.62e+00 | Dis Loss:7.85e-02\n",
            "Epoch: ( 23) (    5/    6) | Gen Loss:3.99e+00 | Dis Loss:8.66e-02\n",
            "Epoch: ( 23) (    6/    6) | Gen Loss:4.22e+00 | Dis Loss:1.11e-01\n",
            "* Epoch 24/40\n",
            "Epoch: ( 24) (    1/    6) | Gen Loss:3.98e+00 | Dis Loss:1.28e-01\n",
            "Epoch: ( 24) (    2/    6) | Gen Loss:2.84e+00 | Dis Loss:2.21e-01\n",
            "Epoch: ( 24) (    3/    6) | Gen Loss:3.14e+00 | Dis Loss:9.65e-02\n",
            "Epoch: ( 24) (    4/    6) | Gen Loss:3.65e+00 | Dis Loss:1.71e-01\n",
            "Epoch: ( 24) (    5/    6) | Gen Loss:3.81e+00 | Dis Loss:1.48e-01\n",
            "Epoch: ( 24) (    6/    6) | Gen Loss:3.49e+00 | Dis Loss:9.05e-02\n",
            "* Epoch 25/40\n",
            "Epoch: ( 25) (    1/    6) | Gen Loss:3.68e+00 | Dis Loss:1.43e-01\n",
            "Epoch: ( 25) (    2/    6) | Gen Loss:3.91e+00 | Dis Loss:1.92e-01\n",
            "Epoch: ( 25) (    3/    6) | Gen Loss:3.74e+00 | Dis Loss:8.78e-02\n",
            "Epoch: ( 25) (    4/    6) | Gen Loss:4.66e+00 | Dis Loss:7.68e-02\n",
            "Epoch: ( 25) (    5/    6) | Gen Loss:3.81e+00 | Dis Loss:1.12e-01\n",
            "Epoch: ( 25) (    6/    6) | Gen Loss:4.25e+00 | Dis Loss:1.29e-01\n",
            "* Epoch 26/40\n",
            "Epoch: ( 26) (    1/    6) | Gen Loss:4.81e+00 | Dis Loss:1.55e-01\n",
            "Epoch: ( 26) (    2/    6) | Gen Loss:3.65e+00 | Dis Loss:1.29e-01\n",
            "Epoch: ( 26) (    3/    6) | Gen Loss:4.54e+00 | Dis Loss:7.68e-02\n",
            "Epoch: ( 26) (    4/    6) | Gen Loss:4.23e+00 | Dis Loss:3.97e-02\n",
            "Epoch: ( 26) (    5/    6) | Gen Loss:2.17e+00 | Dis Loss:3.75e-01\n",
            "Epoch: ( 26) (    6/    6) | Gen Loss:3.00e+00 | Dis Loss:5.53e-02\n",
            "* Epoch 27/40\n",
            "Epoch: ( 27) (    1/    6) | Gen Loss:3.37e+00 | Dis Loss:1.41e-01\n",
            "Epoch: ( 27) (    2/    6) | Gen Loss:3.22e+00 | Dis Loss:4.06e-02\n",
            "Epoch: ( 27) (    3/    6) | Gen Loss:3.27e+00 | Dis Loss:1.31e-01\n",
            "Epoch: ( 27) (    4/    6) | Gen Loss:4.22e+00 | Dis Loss:4.50e-02\n",
            "Epoch: ( 27) (    5/    6) | Gen Loss:3.22e+00 | Dis Loss:1.62e-01\n",
            "Epoch: ( 27) (    6/    6) | Gen Loss:2.90e+00 | Dis Loss:2.04e-01\n",
            "* Epoch 28/40\n",
            "Epoch: ( 28) (    1/    6) | Gen Loss:2.91e+00 | Dis Loss:8.39e-02\n",
            "Epoch: ( 28) (    2/    6) | Gen Loss:2.77e+00 | Dis Loss:1.67e-01\n",
            "Epoch: ( 28) (    3/    6) | Gen Loss:3.52e+00 | Dis Loss:1.90e-01\n",
            "Epoch: ( 28) (    4/    6) | Gen Loss:3.38e+00 | Dis Loss:1.92e-01\n",
            "Epoch: ( 28) (    5/    6) | Gen Loss:3.01e+00 | Dis Loss:1.76e-01\n",
            "Epoch: ( 28) (    6/    6) | Gen Loss:3.12e+00 | Dis Loss:1.51e-01\n",
            "* Epoch 29/40\n",
            "Epoch: ( 29) (    1/    6) | Gen Loss:4.44e+00 | Dis Loss:1.27e-01\n",
            "Epoch: ( 29) (    2/    6) | Gen Loss:3.06e+00 | Dis Loss:1.36e-01\n",
            "Epoch: ( 29) (    3/    6) | Gen Loss:3.17e+00 | Dis Loss:1.36e-01\n",
            "Epoch: ( 29) (    4/    6) | Gen Loss:2.98e+00 | Dis Loss:2.21e-01\n",
            "Epoch: ( 29) (    5/    6) | Gen Loss:3.90e+00 | Dis Loss:1.07e-01\n",
            "Epoch: ( 29) (    6/    6) | Gen Loss:2.99e+00 | Dis Loss:1.56e-01\n",
            "* Epoch 30/40\n",
            "Epoch: ( 30) (    1/    6) | Gen Loss:3.53e+00 | Dis Loss:6.24e-02\n",
            "Epoch: ( 30) (    2/    6) | Gen Loss:3.02e+00 | Dis Loss:2.34e-01\n",
            "Epoch: ( 30) (    3/    6) | Gen Loss:3.63e+00 | Dis Loss:4.39e-02\n",
            "Epoch: ( 30) (    4/    6) | Gen Loss:3.77e+00 | Dis Loss:7.93e-02\n",
            "Epoch: ( 30) (    5/    6) | Gen Loss:3.72e+00 | Dis Loss:1.14e-01\n",
            "Epoch: ( 30) (    6/    6) | Gen Loss:4.67e+00 | Dis Loss:1.36e-01\n",
            "* Epoch 31/40\n",
            "Epoch: ( 31) (    1/    6) | Gen Loss:4.80e+00 | Dis Loss:1.31e-01\n",
            "Epoch: ( 31) (    2/    6) | Gen Loss:4.27e+00 | Dis Loss:8.00e-02\n",
            "Epoch: ( 31) (    3/    6) | Gen Loss:2.96e+00 | Dis Loss:8.97e-02\n",
            "Epoch: ( 31) (    4/    6) | Gen Loss:3.19e+00 | Dis Loss:1.42e-01\n",
            "Epoch: ( 31) (    5/    6) | Gen Loss:3.88e+00 | Dis Loss:5.87e-02\n",
            "Epoch: ( 31) (    6/    6) | Gen Loss:4.48e+00 | Dis Loss:1.02e-01\n",
            "* Epoch 32/40\n",
            "Epoch: ( 32) (    1/    6) | Gen Loss:3.45e+00 | Dis Loss:9.48e-02\n",
            "Epoch: ( 32) (    2/    6) | Gen Loss:2.86e+00 | Dis Loss:5.86e-02\n",
            "Epoch: ( 32) (    3/    6) | Gen Loss:3.54e+00 | Dis Loss:2.63e-01\n",
            "Epoch: ( 32) (    4/    6) | Gen Loss:3.43e+00 | Dis Loss:1.21e-01\n",
            "Epoch: ( 32) (    5/    6) | Gen Loss:2.97e+00 | Dis Loss:1.98e-01\n",
            "Epoch: ( 32) (    6/    6) | Gen Loss:4.73e+00 | Dis Loss:9.67e-02\n",
            "* Epoch 33/40\n",
            "Epoch: ( 33) (    1/    6) | Gen Loss:3.34e+00 | Dis Loss:1.11e-01\n",
            "Epoch: ( 33) (    2/    6) | Gen Loss:4.36e+00 | Dis Loss:1.30e-01\n",
            "Epoch: ( 33) (    3/    6) | Gen Loss:3.92e+00 | Dis Loss:5.40e-02\n",
            "Epoch: ( 33) (    4/    6) | Gen Loss:3.34e+00 | Dis Loss:1.64e-01\n",
            "Epoch: ( 33) (    5/    6) | Gen Loss:1.97e+00 | Dis Loss:3.17e-01\n",
            "Epoch: ( 33) (    6/    6) | Gen Loss:2.74e+00 | Dis Loss:1.87e-01\n",
            "* Epoch 34/40\n",
            "Epoch: ( 34) (    1/    6) | Gen Loss:6.58e+00 | Dis Loss:2.28e-01\n",
            "Epoch: ( 34) (    2/    6) | Gen Loss:2.70e+00 | Dis Loss:3.50e-01\n",
            "Epoch: ( 34) (    3/    6) | Gen Loss:2.53e+00 | Dis Loss:1.39e-01\n",
            "Epoch: ( 34) (    4/    6) | Gen Loss:2.74e+00 | Dis Loss:1.70e-01\n",
            "Epoch: ( 34) (    5/    6) | Gen Loss:4.11e+00 | Dis Loss:1.04e-01\n",
            "Epoch: ( 34) (    6/    6) | Gen Loss:2.65e+00 | Dis Loss:1.36e-01\n",
            "* Epoch 35/40\n",
            "Epoch: ( 35) (    1/    6) | Gen Loss:3.68e+00 | Dis Loss:5.21e-02\n",
            "Epoch: ( 35) (    2/    6) | Gen Loss:3.92e+00 | Dis Loss:1.91e-01\n",
            "Epoch: ( 35) (    3/    6) | Gen Loss:2.59e+00 | Dis Loss:1.49e-01\n",
            "Epoch: ( 35) (    4/    6) | Gen Loss:4.33e+00 | Dis Loss:9.52e-02\n",
            "Epoch: ( 35) (    5/    6) | Gen Loss:3.95e+00 | Dis Loss:7.87e-02\n",
            "Epoch: ( 35) (    6/    6) | Gen Loss:3.24e+00 | Dis Loss:4.80e-02\n",
            "* Epoch 36/40\n",
            "Epoch: ( 36) (    1/    6) | Gen Loss:3.31e+00 | Dis Loss:1.26e-01\n",
            "Epoch: ( 36) (    2/    6) | Gen Loss:3.71e+00 | Dis Loss:7.28e-02\n",
            "Epoch: ( 36) (    3/    6) | Gen Loss:3.67e+00 | Dis Loss:6.71e-02\n",
            "Epoch: ( 36) (    4/    6) | Gen Loss:3.89e+00 | Dis Loss:1.12e-01\n",
            "Epoch: ( 36) (    5/    6) | Gen Loss:4.13e+00 | Dis Loss:9.74e-02\n",
            "Epoch: ( 36) (    6/    6) | Gen Loss:3.43e+00 | Dis Loss:2.01e-01\n",
            "* Epoch 37/40\n",
            "Epoch: ( 37) (    1/    6) | Gen Loss:3.71e+00 | Dis Loss:6.71e-02\n",
            "Epoch: ( 37) (    2/    6) | Gen Loss:3.67e+00 | Dis Loss:7.91e-02\n",
            "Epoch: ( 37) (    3/    6) | Gen Loss:2.34e+00 | Dis Loss:1.87e-01\n",
            "Epoch: ( 37) (    4/    6) | Gen Loss:3.83e+00 | Dis Loss:8.95e-02\n",
            "Epoch: ( 37) (    5/    6) | Gen Loss:4.04e+00 | Dis Loss:3.12e-01\n",
            "Epoch: ( 37) (    6/    6) | Gen Loss:3.17e+00 | Dis Loss:1.40e-01\n",
            "* Epoch 38/40\n",
            "Epoch: ( 38) (    1/    6) | Gen Loss:3.78e+00 | Dis Loss:8.60e-02\n",
            "Epoch: ( 38) (    2/    6) | Gen Loss:4.73e+00 | Dis Loss:8.60e-02\n",
            "Epoch: ( 38) (    3/    6) | Gen Loss:3.25e+00 | Dis Loss:2.54e-01\n",
            "Epoch: ( 38) (    4/    6) | Gen Loss:3.63e+00 | Dis Loss:6.82e-02\n",
            "Epoch: ( 38) (    5/    6) | Gen Loss:3.05e+00 | Dis Loss:1.12e-01\n",
            "Epoch: ( 38) (    6/    6) | Gen Loss:3.17e+00 | Dis Loss:1.03e-01\n",
            "* Epoch 39/40\n",
            "Epoch: ( 39) (    1/    6) | Gen Loss:4.61e+00 | Dis Loss:1.50e-01\n",
            "Epoch: ( 39) (    2/    6) | Gen Loss:3.83e+00 | Dis Loss:6.81e-02\n",
            "Epoch: ( 39) (    3/    6) | Gen Loss:3.76e+00 | Dis Loss:4.96e-02\n",
            "Epoch: ( 39) (    4/    6) | Gen Loss:3.39e+00 | Dis Loss:1.14e-01\n",
            "Epoch: ( 39) (    5/    6) | Gen Loss:3.33e+00 | Dis Loss:6.30e-02\n",
            "Epoch: ( 39) (    6/    6) | Gen Loss:2.90e+00 | Dis Loss:1.49e-01\n",
            "* Epoch 40/40\n",
            "Epoch: ( 40) (    1/    6) | Gen Loss:2.75e+00 | Dis Loss:1.27e-01\n",
            "Epoch: ( 40) (    2/    6) | Gen Loss:5.20e+00 | Dis Loss:2.93e-01\n",
            "Epoch: ( 40) (    3/    6) | Gen Loss:3.61e+00 | Dis Loss:2.19e-01\n",
            "Epoch: ( 40) (    4/    6) | Gen Loss:4.13e+00 | Dis Loss:8.96e-02\n",
            "Epoch: ( 40) (    5/    6) | Gen Loss:3.36e+00 | Dis Loss:2.65e-01\n",
            "Epoch: ( 40) (    6/    6) | Gen Loss:3.49e+00 | Dis Loss:5.37e-02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybm-W7lSDASi",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на прекрасный график"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhosyMilmSC-",
        "colab_type": "code",
        "outputId": "e289008c-4e35-4b3c-9cdc-c9aa05bd8a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(gen_losses)), gen_losses, label='gen loss')\n",
        "plt.plot(range(len(discA_losses)), discA_losses, label='discA loss')\n",
        "plt.plot(range(len(discB_losses)), discB_losses, label='discB loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpPQLhkSDGBj",
        "colab_type": "text"
      },
      "source": [
        "Ну и, наконец, получим результат"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k6kJ46YrJ5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_real_test = Variable(iter(people_loader).next()[0], requires_grad=True)\n",
        "b_real_test = Variable(iter(hodocs_loader).next()[0], requires_grad=True)\n",
        "a_real_test, b_real_test = a_real_test.to(device), b_real_test.to(device)\n",
        "        \n",
        "\n",
        "genA.eval()\n",
        "genB.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    a_fake_test = genA(b_real_test)\n",
        "    b_fake_test = genB(a_real_test)\n",
        "    a_recon_test = genA(b_fake_test)\n",
        "    b_recon_test = genB(a_fake_test)\n",
        "\n",
        "pic = (torch.cat([a_real_test, b_fake_test, a_recon_test, b_real_test, a_fake_test, b_recon_test], dim=0).data+0.5) / 2.0\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4npVRIROrvwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3PKo_FFDOfS",
        "colab_type": "text"
      },
      "source": [
        "Можете создать отдельную папку p2h_results и сохранить туда"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN8taMTtxfIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torchvision.utils.save_image(pic, 'gdrive/My Drive/p2h_results/leaky_got_aug_unet_sample_result'+str(epochs)+'.jpg', nrow=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkVHf6jIDYpK",
        "colab_type": "text"
      },
      "source": [
        "Или просто к корневую"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97VYFPg-9GIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torchvision.utils.save_image(pic, 'gdrive/My Drive/leaky_got_aug_unet_sample_result'+str(epochs)+'.jpg', nrow=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHauNaNSjhBo",
        "colab_type": "text"
      },
      "source": [
        "# Фанатам Игры Престолов посвещается"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugLhUXIHDbtG",
        "colab_type": "text"
      },
      "source": [
        "Ну и в завершение скажу, что пока сеть работает не идельно, но проблема лишь в небольшом датасетею При его расширении результат будет гораздо круче. Поэтому всех фанатов игры престолов призываю стать контрибьютерами)\n",
        "Уже охвачены - все подходящие картинки из гугла и серия 8 сезона, где происходит битва с ходоками. Всем желающим пересмтореть шедевр и пособирать скриншоты для этого проекта буду рада!"
      ]
    }
  ]
}